% Appendix D

\chapter{Transfer learning} % Main appendix title
\chaptermark{Transfer learning}
\label{AppendixE} % For referencing this appendix elsewhere, use \ref{AppendixA}

%\lhead{Appendix C. \emph{Statistical Tests}} % This is for the header on each page - perhaps a shortened title

\section{Learning transfer from the a benchmark dataset}

We also explored the possibility of applying transfer learning to our problem. We trained the multi-scale CNN (MCNN) from \cite{godinez2017multi} on the BBBC21v2 dataset with 13 MOA classes (including DMSO), validating it at $96.25\%$ accuracy on balanced data. For comparison, \cite{godinez2017multi} achieved $96/103 \approx 93\%$ on the ultimate MOA prediction problem. We made minor modifications to the MCNN design specification including adding dropout layers (\cite{srivastava2014dropout}) to fight overfitting. The network is of similar depth to the classical LeNet (\cite{lecun1998gradient}) but has multiple pathways for each rescaling of its inputs and vast fully-connected layers after a concatenation layer. As it is trained on full-sized images ($1040 \times 1280$px), training on a single GPU for 20 epochs takes of the order of 24 hours. Once trained, we fine-tuned by replacing the softmax layer to have 8 neurons and running stochastic gradient descent with a lowered learning rate of $10^{-4}$  with momentum term $0.9$. Due to the compute expense in our usual LOCOCV strategy, we opt for a different approach. Thus, we trained on the four fields of 32 wells, and predicted on the remaining 8 (one per MOA class). With this we achieved XXX



%\begin{table}
%\begin{tabular}{|l|l|} 
%\hline
%Approach & Pooled cell line accuracy ($\mu$, $\sigma$)\\ \hline
%Conv. autoencoder & $19.58, 6.98$  \\ \hline
%Multitask conv. autoencoder & $20.42, 6.14$ \\
%Domain-adversarial conv. autoencoder & $\mathbf{22.38, 5.91}^{**}$ \\ \hline
%\end{tabular}
%\caption{MOA prediction on multiple cell lines (pooled) with convolutional autoencoders. From top to bottom: vanilla convolutional autoencoders (baseline), multitask convolutional autoencoders and domain-adversarial convolutional autoencoders. We compare with the vanilla convolutional autoencoder (top row) ($(^{**})$ : $p<0.01$).}
%\label{table:joint_deep}
%\end{table}
%
%
%\begin{table}[!t]
%\caption{Comparison of performance for models arranged by profile criterion category. \label{Tab:01}} {\begin{tabular}{@{}lllll@{}}\toprule Property & Approach & MDA231 accuracy ($\mu, \sigma$) & MDA468 accuracy ($\mu, \sigma$) & Joint accuracy ($\mu, \sigma$) \\\midrule
%Unit of measurement & Cell bounding box & 29.67, 6.35 & 23.63, 6.49 & 34.42, 7.02 \\
% & Grid ($4\times6$) & 31.04, 7.12 & 18.86, 5.71 & 30.5, 5.64 \\
% & Grid ($10\times12$) & \color{red}{31.20, 6.41} & \color{red}{27.08, 6.13} & 34.42, 6.35 \\
% & Full image & 25.33, 6.49 & 16.08, 3.91 & 34.63, 6.66 \\\midrule
%Feature representation & Handcrafted & 18.58, 5.66 & 20.08, 4.53 & 21.63, 5.66 \\
% & ResNet50 & 29.67, 6.35 & 23.63, 6.49 & 34.42, 7.02 \\\midrule
%Dimensionality reduction & PCA + whitening & 18.58, 5.62 & 20.08, 4.49 & - \\
% & Hierarchical clustering & 11.77, 5.65 & 16.71, 6.63 & - \\
% & K-means & 15.42, 6.73 & 16.71, 6.79 & - \\
% & GMM & 15.73, 6.63 & 17.23, 6.24 & - \\
% & Convolutional autoencoder & 19.13, 6.96 & 14.21, 5.41 & - \\
% & Multiple-instance learning & 19.51, 9.95 & 16.81, 8.16 & - \\
% & MCNN & 12.29, 11.15 & 12.08, 12.71 & \\
% & Autoencoder & 10.56, 5.07 & 15.58, 6.67 & 28.13, 7.30 \\
% & Multi-input autoencoder & - & - & 31.21, 6.63 \\
% & Multitask autoencoder & - & - & 32.04, 6.88 \\
% & Domain-adversarial autoencoder & & & \color{red}{34.67, 6.93} \\\midrule
%Aggregation strategy & K-S test & 19.08, 5.09 & 20.17, 6.21 & - \\
% & Averaging & 18.58, 5.66 & 20.08, 4.53 & 21.63, 5.66 \\
% & Norm to hyperplane & 17.38, 4.99 & 17.40, 5.26 & - \\\botrule
%\end{tabular}}{}
%\label{table:joint}
%\end{table*}

\section{Pre-trained models}

We also extract pre-trained features from segmented cells. To do this, we take image crops of cells formed by padding the bounding boxes of the segmented nuclei to a common size ($128\times 128$px) and performing bilinear downsampling to produce $32 \times 32 \times 3$ inputs. We also use these as the inputs to the convolutional autoencoder. We use the final convolutional layer of ResNet50 pre-trained on ImageNet data as a feature extractor on the same crops, and also patches of fixed size. This produces a CNN code feature vector of $2048$ elements for each cell. In addition, we study the performance of segmentation-free approaches. First we apply the same network to patches extracted on a $10 \times 12$ grid, corresponding to the average number of cells per field of view over all datasets (115). We also apply the network to larger network inputs: $224 \times 224$px resized versions of crops taken over a $4 \times 6$ grid (corresponding to the approximate aspect ratio of our images ($1040 \times 1392$px), and $224 \times 224$px resizings of the full image. In these latter two cases, we rather use the penultimate layer of features of ResNet50, also yielding a $2048$-dimensional vector per unit. We provide a comparison of these in Table \ref{table:units}. We find that the segmentation-based approach with  outperforms all three of the segmentation-free approaches. Note that we also run a segmentation-free analysis with MCNN in Section \ref{subsubsec:dim_red}.
