% Chapter Template

\chapter{Conclusions} % Main chapter title

\label{Chapter7} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

Computational phenotyping is an ascendant methodology for the extraction of information from biological matter. This thesis has explored a range of solutions for two problems in computational phenotyping. Despite the myriad overlaps, these problems ultimately sit on different axes of development. Part \ref{partI} explored ways of incorporating multiple biological models of different disease subtypes in a joint study. The molecular heterogeneity of these multiple cell lines led ultimately to a computational problem that we resolved with domain adaptation. Part \ref{partII} made attempts at fluorescence-free phenotyping by leveraging a dataset pairing phase contrast and fluorescence microscopy images. Part \ref{partI} can be considered as a stepping stone for computational phenotyping in the long road to precision medicine, and as such remains open-ended. On the other hand, Part \ref{partII} belongs to a more immediate trend for which it is easier to see an end, namely, that deep learning may soon revolutionise the microscopy tool set for conducting biological experiments.

In the following we first give a summary of the chapters and finally speculate about the future of high content screening.

\section{Chapter summaries}

Chapter \ref{Chapter3} introduced the high content analysis pipeline and applied it to a drug and morphological screen datasets for multiple triple-negative breast cancer (TNBC) cell lines. We produced a series of use cases spanning each of univariate-, multivariate-, and machine learning-based analyses. While these were included partly as motivating examples, we were in particular able to highlight a systematic bias in a standard piece of analysis using causal inference formalisms. We were also able to observe a first instance of the differential effects of drugs on cell lines in the form of viability, which we were additionally able to categorise by drug mechanism of action. We additionally saw how TNBC cell lines could be analysed in unison, and explored their phenotypic differences across two axes of measurement: firstly at the population level, as different molecular subtypes manifested distinct patterns of cell cycle and aberrant morphologies; secondly at the cellular level, as the fundamental wild type morphologies were easily distinguishable by a simple classifier. The sum of these insights were carried forward the following chapter.

In Chapter \ref{Chapter4} we explored in great detail the construction of phenotypic profiles for charactersing drug effects. We reviewed and tested the most relevant existing approaches in a comparative study on mechanism of action (MOA) prediction. We then extended the methodology to a multi-cell-line drug screen, where we were able to show an autoencoder based on unsupervised domain adaptation could succeed in building domain-invariant features that outperformed baseline models in MOA prediction. We additionally found evidence to suggest that pooled multi-cell-line data provides more powerful representations for predicting the MOA of hold-out drugs than single-cell line datasets of the same size. In contrast with previous studies, we framed a use case for phenotypic profiling without tailoring the prediction problem to those MOAs with the most striking visual effects, yet confirmed MOA classification could be performed at well above a rate of random chance. This dataset has since been released in full to the scientific community, to our knowledge the first of its kind. In sum, this chapter validated a novel solution for handling the heterogeneity of multi-cell-line screens using deep and adversarial learning.

Chapter \ref{Chapter5} compared two deep learning methodologies for quantifying CAR-T and Raji cell line cells in time lapse movies. With a setup providing a paired image dataset of transmitted light and fluorescent channels, the key barrier for training deep phenotypers is toppled. Each methodology explored a different use of fluorescence for automatically training a deep neural net. In doing so, the trained models progress towards replacing the need for fluorescence in the first place. The first approach, based on image-to-image translation networks, including a generative adversarial variant, showed excellent results in the fluorescent labeling of cell populations for mCherry and GFP markers. Though the path to full quantification of cells remains uncertain, this already works well as a visualisation tool. The second showed how a training set for object detection could be built, and that this is a viable alternative to fluorescence labeling, that moreover gets to the heart of the problem of phenotyping cells in time series. In a final result, we were able to reconcile the two methodologies and demonstrate the degree of agreement between them over a set of experimental replicates.

In the final Chapter \ref{Chapter6}, we sought a way to transcend the limitations of automatic ground truth assemblage encountered in Chapter \ref{Chapter5}. Here we found generative adversarial networks produce excellent results in the synthesis of individual cells, including when conditioned on a class indicator variable. From there, we were inspired to syntheisise full populations of cells, by formulating a style transfer problem, where style was grafted from a curated ground truth image to a content image of our choosing. The content image was constructed to emulate the properties of cells and cell populations. A feasibility study in classical style transfer demonstrated the validity of this concept. This was superseded by a learning approach to unpaired image-to-image translation, the CycleGAN, which conditioned one of its generators on the content image. Using this generator as an oracle for training data, we were able to train a state-of-the-art object detection system. In a final section, we sketched an extension to the CycleGAN so as to be able to specify class information in the content specification.

\section{The future of high content screening}

The future of high content screening lies plausibly in the data-driven domain of deep learning, to which so many areas of computer vision have yielded in recent years. Chapter \ref{Chapter5} in particular showed how seamlessly deep convolutional networks can be assimilated into the screening process. These applications are arguably low-hanging fruit, however, and make an important yet singular contribution to screening process. A deeper question is how to improve the screening process altogether. Researchers such as \cite{kraus2016classifying}, \cite{kandaswamy2016high}, \cite{godinez2017multi}, and \cite{sommer2017deep} have shown how the conventional pipeline can be partially or fully subsumed to a neural network optimised end-to-end. However, their impact has so far been rather superficial.

The main bottleneck, as always, is data. Most chapters in this thesis were about making intelligent use of a relatively limited data supply: first by domain adaptation in Chapter \ref{Chapter4}, then by leveraging fluorescence as annotation in \ref{Chapter5}, and finally augmenting the available data with generative models and style transfer in Chapter \ref{Chapter6}. In the long run, however, there is no replacement for a vastly increased data supply. After decades spent dormant, the power of deep learning was finally unleashed by large, general-purpose image datasets like ImageNet (\cite{russakovsky2015imagenet}). A highly valuable discovery was that upon training neural networks a first time, they could redeployed to exceed the state-of-the-art of yesteryear on virtually any vision problem. This is the power of transfer learning and fine-tuning.

High content screens become different very quickly, however. For one, the number and kind of fluorescent channels varies from screen to screen, as they are chosen for different sub-cellular regions of interest. As a result, intensity colocalisation across fluorescent channels generally have a very different meaning from screen to screen. There is therefore significant interest in models that perform across assays and microscopy modalities (for example, \cite{hollandi2019deep} for cell segmentation).

While sizeable screening image sets exist for drug MOA prediction, their generality remains dubious. It remains that by using the conventional approach, (that is, classical image analysis features), the simplest imaginable classifier can bulldoze the benchmark MOA prediction task (\cite{singh2014pipeline}). Our own drug screen (Part \ref{partI}) likely provides a more realistic picture, with drugs. Our results, though far better than random, are considerably more modest. There is no doubt, however, that if this pilot screen were followed up with a full primary screen, with more cell lines and a broader drug panel, the power of our approach would be greatly enhanced. A full primary screen would likely mean a $1$-$2$ order of magnitude increase in the data volume available for learning (up to around one terrabyte).

But what if the data volume were increased by another few orders of magnitude again?


Chapter \ref{Chapter4} confirmed the rationality of multi-cell-line high content screens.
