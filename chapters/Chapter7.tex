% Chapter Template

\chapter{Conclusions} % Main chapter title

\label{Chapter7} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

Computational phenotyping is an ascendant methodology for the study of phenotypes. This thesis has explored a range of solutions for two problems in computational phenotyping. Despite the manifest overlap, these problems ultimately sit on different axes of development. Part \ref{partI} explored ways of performing a joint study of multiple disease subtypes. The molecular heterogeneity of multiple cell lines led ultimately to a computational problem that was resolved with domain adaptation. Part \ref{partII} compared approaches to fluorescence-free phenotyping by leveraging a dataset pairing phase contrast and fluorescence microscopy images. Part \ref{partI} can be considered a stepping stone for computational phenotyping in the long road to precision medicine, and as such remains an open-ended study. On the other hand, Part \ref{partII} belongs to a recent trend in biomedical imaging, for which a more immediate outcome is likely, namely, that deep learning may soon revolutionise the microscopy tool set for conducting biological experiments. The following provides first a summary of the chapters and then speculation about the future of high content screening.

\section{Chapter summaries}

Chapter \ref{Chapter3} introduced the high content analysis pipeline and applied it to drug and wild type screen datasets for multiple triple-negative breast cancer (TNBC) cell lines. A series of use cases were presented spanning each of the univariate-, multivariate-, and machine learning-based analyses formalised in Section \ref{subsec:HCA}. While these were included as motivating examples, we saw in particular an analysis of double strand breaks that highlighted a systematic bias in a standard piece of analysis using causal inference formalisms. Elsewhere we observed a first instance of the differential effects of drugs on cell lines in the form of viability, which were additionally categorised by drug mechanism of action. We also saw how TNBC cell lines could be analysed in unison, and explored their phenotypic differences across two axes of measurement: firstly at the population level, as different molecular subtypes manifested distinct patterns of cell cycle and aberrant morphologies; then at the cellular level, with the fundamental wild type morphologies easily distinguishable by a simple classifier. The sum of these insights was carried forward to the following chapter.

Chapter \ref{Chapter4} explored in great detail the construction of phenotypic profiles for charactersing drug effects. It reviewed and tested the most relevant existing approaches in a comparative study of mechanism of action (MOA) prediction. It then extended the methodology to a multi-cell-line setting, where it was shown that an autoencoder based on unsupervised domain adaptation could succeed in building domain-invariant features that outperform baseline models in MOA prediction. We additionally found evidence to suggest that pooled multi-cell-line data provides more powerful representations for predicting the MOA of hold-out drugs than single-cell line datasets of the same size. In contrast with previous studies, this use case for phenotypic profiling was framed without tailoring the prediction problem to those MOAs with the most striking visual effects, yet confirmed MOA classification could be performed at well above a rate of random chance. This dataset has since been released in full to the scientific community. In sum, this chapter validated a novel solution for managing the heterogeneity of multi-cell-line screens using deep and adversarial learning.

Chapter \ref{Chapter5} compared two deep learning methodologies for quantifying CAR-T and Raji cell line cells in time lapse movies. With a setup providing a paired image dataset of transmitted light and fluorescent channels, the key barrier for training deep phenotypers is toppled. Each methodology explored a different use of fluorescence for automatically training deep neural networks. In doing so, the trained models progressed towards supplanting fluorescence altogether. The first approach, based on image-to-image translation networks, including a conditional GAN, showed excellent results in the fluorescent labeling of cell populations for mCherry and GFP markers. Though the path to full quantification of cells remains uncertain, this already works well as a visualisation tool. The second showed how a training set for object detection could be built, and that this is a viable alternative to fluorescence labeling, that moreover gets to the heart of the problem of counting cell phenotypes in time series. In a final result, we were able to reconcile the two methodologies and demonstrate a high degree of correlation across a set of experimental replicates.

Chapter \ref{Chapter6} sought a way to transcend the limitations of automatic ground truth assemblage encountered in Chapter \ref{Chapter5}. Here it was shown that generative adversarial networks produce excellent results in the synthesis of individual cells, including when conditioned on a class indicator variable. This inspired the synthesis of full populations of cells, formulated as a problem of style transfer, where style was grafted from a curated ground truth image to a chosen content image. The content image was constructed to emulate the geometric characteristics of cells and cell populations. A feasibility study in classical style transfer demonstrated the validity of this concept. This was superseded by a learning approach to unpaired image-to-image translation, the CycleGAN, which conditioned one of its generators on the content image. Using this generator as an oracle for training data, a state-of-the-art object detection system was trained, which was shown to outperform a baseline of weakly annotated ground truth data. In a final section, an extension to the CycleGAN was conceptualised, so as to be able to inject cell class information in the content specification as well.

\section{The future of high content screening}

% Deep Learning: how can we apply dl to bioimaging
The future of high content screening lies plausibly in the data-driven domain of deep learning, to which so many areas of computer vision have yielded in recent years. After decades spent dormant, the power of deep learning was finally unleashed by large, annotated datasets of natural images like ImageNet (\cite{russakovsky2015imagenet}). In high content screening, the enthusiasm generated by the stunning results provided by deep learning were attenuated by two factors: first, the results obtained by other machine learning learning methods were already very good, and often sufficient to answer the biological questions studied. Second, the amount of annotations to provide was prohibitory.

\subsection{Overcoming massive image annotation}

Most chapters in this thesis were about making intelligent use of a relatively limited data supply: first by domain adaptation in Chapter \ref{Chapter4}; then by leveraging fluorescence as annotation in \ref{Chapter5}; and finally augmenting the available data with style transfer and generative models in Chapter \ref{Chapter6}. In more general terms, there are several ways to overcoming the bottleneck imposed by annotation:

\begin{itemize}
	\item In image segmentation, the size of annotated data sets required to train deep neural networks is not excruciatingly large. If the imaging conditions are coherent (which is normally the case in HCS), a few fully annotated images are actually often sufficient (\cite{ronneberger2015u}). Secondly, for common segmentation tasks, such as nuclear segmentation, the community felt that it would be worth going to the trouble of annotating large image datasets, as was the case for the ``Data Science Bowl 2018''\footnote{https://www.kaggle.com/c/data-science-bowl-2018/overview/about}, with $37333$ manually annotated nuclei in $841$ images of different modalities. In this case, the objective was to train a general neural network capable of segmenting nuclei irrespective of the imaging modality. This sparked the development of a number of deep networks that are indeed useful for nuclei segmentation (\cite{hollandi2019deep}). However, it must be noted that this large community effort is so far unique for segmentation problems in light microscopy, which reflects the central importance of nuclear segmentation. It may be summarised that deep learning has become the state of the art in bioimage segmentation and has largely replaced other techniques.
	\item An invaluable discovery in deep learning was that upon training a deep classifier once, it could be redeployed to exceed the state-of-the-art of yesteryear on virtually any vision problem. This is the power of transfer learning and fine-tuning. High content screens become different very quickly, however. For one, the number and kind of fluorescent channels varies from screen to screen, as they are chosen for different sub-cellular regions of interest. As a result, intensity colocalisation across fluorescent channels generally have a very different meaning from screen to screen. Furthermore, this fine-tuning approach has been criticised as being badly adapted to the specific properties of biomedical images and heavily over-parametrised (\cite{raghu2019transfusion}). This is due to fact that the relevant information for classifying biomedical images is often highly localised, as is the case for microcalcifications in mammogram classification (see, for example, \cite{wang2018context}).
	\item Another strategy is for generating large annotated datasets for cell classification, namely the classification of subcellular protein localisation patterns, is crowd sourcing and gamification (\cite{sullivan2018deep}). While this strategy has proven successful, it is also limited to large and prestigious projects with generous funding and is therefore not good solution for routine screens. 
	\item Yet another technique that has been used specifically in screening is weak supervision, where instead of training classifiers for single cell phenotypes, representations are learned by predicting drug identity. Researchers such as \cite{kraus2016classifying}, \cite{kandaswamy2016high}, \cite{godinez2017multi}, and \cite{sommer2017deep} have shown how the conventional pipeline can be partially or fully subsumed to a neural network optimised end-to-end using this approach. This is an interesting approach with high potential in drug screening, because it efficiently removes the burden of massive annotation. However, their impact has so far been rather superficial, primarily improving MOA prediction accuracy on a benchmark dataset by a few percentage points. Furthermore, deep learning ought to allow us to somehow transcend the conventional pipeline altogether. A further problem is that one no longer has a strong interpretability of intermediate steps, as it does not involve the classification of individual cells into biologically meaningful classes.
\end{itemize}

%The renaissance of label-free microscopy
\subsection{The renaissance of label-free microscopy}

The use of deep learning is not limited to improving classification or segmentation accuracies. The advent of image-to-image translation models has opened new avenues for the use of label-free microscopy. Fluorescence microscopy has many significant advantages, as one can specifically highlight particular biomolecules and subcellular structures. It turns out, however, that many subcellular structures leave a fingerprint in completely non-invasive label-free microscopy techniques, such as phase contrast or bright-field microscopy. Whereas several years ago it was still very challenging to quantitatively analyse these label-free microscopy datasets, the tools to do so are now available.

It is therefore possible that we will witness a renaissance of non-invasive label-free microscopy techniques in high content screening, where fluorescence microscopy is used for: (i) \emph{calibration}, that is for the training of neural networks to reconstruct the fluorescence microscopy images from label-free microscopy or else to recognise marked structures; and (ii) \emph{visualisation} of the biomolecules that do not leave a fingerprint in the images. While fluorescence microscopy will remain an indispensable tool for studying cells, the augmented label-free microscopy may become a means of obtaining ``more from less'' from transmitted light microscopies such as phase contrast. Chapter \ref{Chapter5} in particular showed how seamlessly deep convolutional networks can be assimilated into the screening process as fluorescent predictors or cell classifiers trained by cross-modality.

These approaches may turn out to be controversial. Is it still science if we predict measurements instead of taking them? How valid are the conclusions that can be drawn from predicted data? To what extent would a different training set influence the final conclusions? One toolset that can help in overcoming these justified criticisms is a proper statistical treatment of predicted measurements, as recently proposed in \cite{whitehill2018automatic}. Indeed, if statistical tests are performed on experimental conditions where one actually works on predictions rather than direct measurements, one needs ensure also that the uncertainty of the prediction into account. Here there are many methodological developments still to be made.

% Multi-cell-line screens
\subsection{Towards toxicogenetics}

Besides the technical improvements and extensions that are brought by new learning techniques, another important evolution concerns the biological models (in the case of this thesis, the cell lines). Indeed, the screening of entire cell line panels has created the opportunity to develop predictive models that would allow for predicting the phenotype resulting from a drug treatment on a cell line given the chemical features describing the drug, alongside genomic and transcriptomic features describing the cell line. Such datasets mimic a precision medicine use case (\cite{eduati2015prediction, costello2014community}), where we wish to predict a targeted treatment instead of having a one-size-fits-all solution, which might work in a statistical sense at the population level, but not necessarily at the level of an individual patient. Despite this being a promising strategy, and in spite of the efforts of numerous teams working in the field, the success has been relatively modest so far. One of the reasons for this is certainly the representation of a complex drug effect by a single number (IC50, as in \cite{eduati2015prediction}). The approach developed in Chapter \ref{Chapter4} would allow for the analysis of such screens, and in particular for making meaningful comparisons of morphological phenotypes across cell lines with different morphological baselines. This is a cornerstone of the most powerful algorithms in the field, such as kernel methods in multi-task setting (\cite{playe2018efficient}).

% \cite{eduati2015prediction, costello2014community}



% deep learning, another important evolution concerns the biological models used for screening, as  



% % MOA prediction 


% Chapter \ref{Chapter5} in particular showed how seamlessly deep convolutional networks can be assimilated into the screening process as fluorescent labelers. While fluorescence microscopy will remain an indispensable tool for studying cells, these models may become a means of obtaining ``more from less'' from transmitted light microscopies such as phase contrast. 

% Such applications are arguably low-hanging fruit, however, and make an important yet singular contribution to screening process. A deeper question is how to improve the screening process altogether. Researchers such as \cite{kraus2016classifying}, \cite{kandaswamy2016high}, \cite{godinez2017multi}, and \cite{sommer2017deep} have shown how the conventional pipeline can be partially or fully subsumed to a neural network optimised end-to-end. However, their impact has so far been rather superficial, primarily improving MOA prediction accuracy on a benchmark dataset by a few percentage points. Furthermore, deep learning ought to allow us to somehow transcend the conventional pipeline altogether.

% The main bottleneck, as always, is data. Most chapters in this thesis were about making intelligent use of a relatively limited data supply: first by domain adaptation in Chapter \ref{Chapter4}; then by leveraging fluorescence as annotation in \ref{Chapter5}; and finally augmenting the available data with style transfer and generative models in Chapter \ref{Chapter6}. In the long run, however, there is no replacement for a vastly increased data supply. 

% After decades spent dormant, the power of deep learning was finally unleashed by large, general-purpose image datasets like ImageNet (\cite{russakovsky2015imagenet}). An invaluable discovery was that upon training neural networks once, they could redeployed to exceed the state-of-the-art of yesteryear on virtually any vision problem. This is the power of transfer learning and fine-tuning. High content screens become different very quickly, however. For one, the number and kind of fluorescent channels varies from screen to screen, as they are chosen for different sub-cellular regions of interest. As a result, intensity colocalisation across fluorescent channels generally have a very different meaning from screen to screen. There is therefore significant interest in models that perform across assays and microscopy modalities (for example, \cite{hollandi2019deep} for cell segmentation).

% While sizeable screening image sets exist for drug MOA prediction, their generality is dubious. It remains that by using the conventional approach, (that is, classical image analysis features), the simplest imaginable classifier can bulldoze the benchmark MOA prediction task (\cite{singh2014pipeline}). Our own drug screen (Part \ref{partI}) likely paints a more realistic picture, with drugs     . Our results, though far better than random, are considerably more modest. There is no doubt, however, that if this pilot screen were followed up with a full primary screen, with more cell lines and a broader drug panel, the power of our approach would be greatly enhanced. A full primary screen would likely mean a $1$-$2$ order of magnitude increase in the data volume available for learning (up to around one terrabyte).

% But what if the data volume were increased by another few orders of magnitude again? At this point, we would be screening tens of thousands of drugs. \cite{eduati2015prediction}



% Chapter \ref{Chapter4} confirmed the rationality of multi-cell-line high content screens.
